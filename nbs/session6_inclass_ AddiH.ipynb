{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Benchmark classification on ```cifar-10```\n",
    "\n",
    "This notebook builds on what we were doing last week with the handwritten digits from the MNIST dataset.\n",
    "\n",
    "This week, we're working with another famous dataset in computer vision and image processing research - [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path tools\n",
    "import os\n",
    "\n",
    "# data loader\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# machine learning tools\n",
    "# from sklearn.preprocessing import LabelBinarizer not needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# classificatio models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Ross forgot these how embarrassing\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7343d4b5",
   "metadata": {},
   "source": [
    "We're going to load the data using a function from the library ```TensorFlow```, which we'll be looking at in more detail next week. \n",
    "\n",
    "For now, we're just using it to fetch the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b588be73",
   "metadata": {},
   "source": [
    "**Question:** What is the shape of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd02fcbe",
   "metadata": {},
   "source": [
    "Unfortunately, this version of the data set doesn't have explict labels, so we need to create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"airplane\", \n",
    "          \"automobile\", \n",
    "          \"bird\", \n",
    "          \"cat\", \n",
    "          \"deer\", \n",
    "          \"dog\", \n",
    "          \"frog\", \n",
    "          \"horse\", \n",
    "          \"ship\", \n",
    "          \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all the data to greyscale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5391f3",
   "metadata": {},
   "source": [
    "In the following cell, I'm converting all of my images to greyscale and then making a ```numpy``` array at the end.\n",
    "\n",
    "Notice that I'm using something funky here called *[list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de172ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal for loop\n",
    "for x in y:\n",
    "    do_this(x)\n",
    "\n",
    "# list comprehension for loop\n",
    "[do_this(x) for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"red\", \"green\", \"blue\"]\n",
    "uppers = []\n",
    "\n",
    "# normal for loop\n",
    "for col in cols:\n",
    "    upper = col.upper()\n",
    "    uppers.append(upper)\n",
    "\n",
    "# list comprehension\n",
    "uppers = [col.upper() for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98750531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could make a function that does everything you would put inside a loop and then put it in a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_train])\n",
    "X_test_grey = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9703dbdc",
   "metadata": {},
   "source": [
    "Then, we're going to do some simple scaling by dividing by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train_grey)/255.0\n",
    "X_test_scaled = (X_test_grey)/255.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dab8b0c",
   "metadata": {},
   "source": [
    "## The model converges quicker when everything is scaled down. Then the weights are smaller and the right W&B's are found more quickly.\n",
    "The same information is available - but instead of color channels being from 0 to 255, it is between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c141a5e2",
   "metadata": {},
   "source": [
    "Next, we're going to reshape this data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69baae21",
   "metadata": {},
   "source": [
    "This is maybe a bit overkill way to reshape\n",
    "nsamples  50000\n",
    "nx 32\n",
    "ny 32\n",
    "\n",
    "New shape should be nsamples,nx*ny (50000, 1024)\n",
    "Cubessss.\n",
    "So row i is picture i\n",
    "and column i is the value in pixel i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_train_scaled.shape\n",
    "X_train_dataset = X_train_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsamples, nx, ny = X_test_scaled.shape\n",
    "X_test_dataset = X_test_scaled.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bdea84",
   "metadata": {},
   "source": [
    "We define our Logistic Regression classifier as we have done previously. You'll notice that I've set a lot of different parameters here - you can learn more in the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"none\", # if a weights is small - set it to 0 (L1 regularization). But do not use any penalty here\n",
    "                        tol=0.1, # tolerance - how much the weights can change between X iterations before the training stops. If the weights change less than 0.1 the training stops. Other claassifiers can change over x iterations.\n",
    "                        verbose=True, # prints some updates while training\n",
    "                        solver=\"saga\", # optimise the logistic regression. Optimise the way the weights are updated.\n",
    "                        multi_class=\"multinomial\" # not just a binary classifier, but a multi-class classifier\n",
    "                        ).fit(X_train_dataset, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0088251",
   "metadata": {},
   "source": [
    "The solver - the optimisation problem.\n",
    "The loss function.\n",
    "Epoch er hver gang man har g√•et over alle datapunkterne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc10cdb4",
   "metadata": {},
   "source": [
    "We can then print our classification report, using the label names that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79f6d9b4",
   "metadata": {},
   "source": [
    "I've set a couple of different parameters here - you can see more in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "**NB!** This will take a long time to run! On the 32 CPU machine on UCloud, this takes around 30 seconds per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=42,\n",
    "                    hidden_layer_sizes=(64, 10), # 64 nodes in the first layer, 10 nodes in the second layer\n",
    "                    learning_rate=\"adaptive\", # how fast the weights are updated\n",
    "                    early_stopping=True, # stop training if the weights are not changing much\n",
    "                    verbose=True, # print some updates while training\n",
    "                    max_iter=20 # stop training after 20 iterations\n",
    "                    ).fit(X_train_dataset, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f8809d5",
   "metadata": {},
   "source": [
    "Train data\n",
    "Learn loss values and minimise/optimise\n",
    "\n",
    "Validation data (10% random train data)\n",
    "Get accuracy and minimise/optimise\n",
    "\n",
    "Test data\n",
    "Final scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e489977e",
   "metadata": {},
   "source": [
    "Lastly, we can get our classification report as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, \n",
    "                               y_pred, \n",
    "                               target_names=labels)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a5067ab",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Take the code outlined in this notebook and turn it into two separate Python scripts, one which performs Logistic Regression classification and one which uses the MLPClassifier on the ```Cifar10``` dataset.\n",
    "\n",
    "Try to use the things we've spoken about in clas\n",
    "- Requirements.txt\n",
    "- Virtual environment\n",
    "- Setup scripts\n",
    "- Argparse\n",
    "\n",
    "This task is [Assignment 2 for Visual Analytics](https://classroom.github.com/a/KLVvny7d)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
